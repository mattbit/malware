"""Worksheet.

This notebook presents all the results used in the text (figures, estimations).
"""
import networkx as nx
import numpy as np
import pandas as pd

import powerlaw
from tld import get_fld
from tld.exceptions import TldDomainNotFound

import matplotlib.pyplot as plt
plt.style.use("flat_publication")

###########################################################
# %% Load and pre-process the CTU dataset.                #
###########################################################

nf = pd.read_csv("data/ctu-13/capture20110810.binetflow")

# Keep only unidirectional TCP flows of more than 500 bytes.
nf.Dir = nf.Dir.str.strip()
nf = nf[(nf.Proto == "tcp") & (nf.TotBytes >= 500) & (nf.Dir == "->")]

# Discard the traffic between hosts in the local subnet (147.32.0.0/16).
# We only want to evaluate requests going from local hosts to the internet.
nf = nf[nf.SrcAddr.str.startswith("147.32.")
        & (~nf.DstAddr.str.startswith("147.32."))]

###########################################################
# %% Build the network graph based on the traffic flows.  #
###########################################################

flows = nf.loc[:, ("SrcAddr", "DstAddr")]
G = nx.Graph([(src, dst) for _, src, dst in flows.itertuples()])

# We want a connected graph, so we remove the few small isolated components and
# we keep the largest one.
G = G.subgraph(max([c for c in nx.connected_components(G)], key=len))

###########################################################
# %% Let's look at the characteristics of the graph.      #
###########################################################

assert nx.is_connected(G)
assert nx.is_bipartite(G)

degrees = list(dict(G.degree()).values())
fit = powerlaw.Fit(degrees, discrete=True, verbose=False)

ax = fit.plot_pdf(label="Empirical PDF")
fit.power_law.plot_pdf(color="navy", linestyle="--", alpha=0.5, label="Fit")
ax.set_ylabel("Fraction of nodes")
ax.set_xlabel("Node degree")
ax.legend()
plt.show()

print("Alpha = {:.5f}".format(fit.alpha))

# Look at the degree distribution for the two bipartite sets.
#
# A, B = nx.bipartite.sets(G)
#
# degrees_A = list(dict(G.degree(A)).values())
# degrees_B = list(dict(G.degree(B)).values())
# fit_A = powerlaw.Fit(degrees_A, discrete=True, verbose=False)
# fit_B = powerlaw.Fit(degrees_B, discrete=True, verbose=False)
#
# ax = fit_A.plot_pdf(label="Empirical PDF for set A")
# fit_B.plot_pdf(label="Empirical PDF for set B")
# fit_A.power_law.plot_pdf(color="navy", linestyle="--", alpha=0.5, label="Fit")
# fit_B.power_law.plot_pdf(color="navy", linestyle="--", alpha=0.5)
#
# ax.set_ylabel("Fraction of nodes")
# ax.set_xlabel("Node degree")
# ax.legend()
# plt.show()
# print("Alpha_A = {:.5f}\nAlpha_A = {:.5f}".format(fit_A.alpha, fit_B.alpha))

###########################################################
# %% Generate an artificial graph.                        #
###########################################################

p = powerlaw.Power_Law(xmin=1, parameters=[fit.alpha], discrete=True)
values = p.generate_random(200, estimate_discrete=True)
values = np.asarray(values, dtype=np.int)

G_a = nx.Graph()
nx.bipartite.preferential_attachment_graph(values, 0.5, create_using=G_a)
assert nx.is_connected(G)

degrees_a = list(dict(G_a.degree()).values())
fit_a = powerlaw.Fit(degrees_a)
fit_a.alpha


# %%

nx.is_connected(G)
cc = [c for c in nx.connected_components(G)]
G = G.subgraph(cc[0])

A, B = nx.bipartite.sets(G)
len(A)


popularity = nf.groupby("DstAddr").Dur.count()
for ip, pop in popularity.iteritems():
    nf.loc[nf.DstAddr == ip, "DstPop"] = pop

popularity.sort_values(ascending=False).plot(logx=True, logy=True)
nf
nf.groupby("SrcAddr").Dur.count().sort_values(
    ascending=False).plot(logx=True, logy=True)


active = nf.groupby("SrcAddr").State.count().sort_values(
    ascending=False)[:10].index.values
df = nf[nf.SrcAddr.isin(active)]
df
nf[nf.SrcAddr == active[0]].groupby("DstAddr").agg(
    {"State": "count", "DstPop": "first"})

# nf = nf[nf.SrcAddr.isin(hosts)]
nf[nf.SrcAddr == "147.32.84.170"].groupby("DstAddr").agg(
    {"DstPop": "first", "Dur": "count"}).sort_values("DstPop", ascending=False).plot(logx=True, logy=True)
nf[nf.SrcAddr == "147.32.84.164"].groupby("DstAddr").agg(
    {"DstPop": "first", "Dur": "count"}).sort_values("DstPop", ascending=False).plot(logx=True, logy=True)

nf[nf.SrcAddr == hosts[3]].groupby("DstAddr").agg({"DstPop": "first", "Dur": "count"}).sort_values(
    "DstPop", ascending=False).plot(logx=True, logy=True)

nf.groupby("SrcAddr").Dur.count().sort_values(
    ascending=False).plot(logx=True, logy=True)

pd.concat([nf.SrcAddr, nf.DstAddr]).nunique()

hosts = ["147.32.84.165",
         "147.32.84.170",
         "147.32.84.164",
         "147.32.84.134",
         "147.32.87.36",
         "147.32.80.9",
         "147.32.87.11"]


subnet = nf[nf.SrcAddr.str.startswith(
    "147.32.") | nf.DstAddr.str.startswith("147.32.")]
df = subnet.loc[:, ("SrcAddr", "DstAddr", "TotPkts")
                ].groupby(["SrcAddr", "DstAddr"]).sum()
G = nx.Graph()
for (src, dst), pkts in df.itertuples():
    if G.has_edge(dst, src):
        G.edges[dst, src]["w"] += pkts
    else:
        G.add_edge(src, dst, w=pkts)


# %%


def extract_domain(url):
    try:
        return get_fld(url, fix_protocol=True)
    except Exception:
        return url


df = pd.concat([pd.read_json("data/2009-11-{:02}.json".format(i), lines=True)
                for i in range(1, 23)])
df["domain"] = df.to.apply(extract_domain)
cts = df.groupby("domain")["count"].sum()
cts = cts.sort_values(ascending=False)
cts.reset_index()
cts.shape
cts.plot(logx=True, logy=True)

# Bootstrapping
sample = df.domain.repeat(df["count"])
bootstrap = sample.sample(10000, replace=True)
bootstrap = pd.DataFrame(bootstrap)
bootstrap["request_count"] = 1
_cts = bootstrap.groupby("domain").request_count.count()
_cts = _cts.sort_values(ascending=False)
_cts.plot(logx=True, logy=True)


# HEADERS = ("time", "src", "dst", "protocol",
#            "http_host", "dns_host", "sni_host")
#
# df = pd.read_csv("../data/all.csv", header=None, names=HEADERS)
#
#
# def domain_name(record):
#     for field in ["http_host", "sni_host", "dns_host"]:
#         if not pd.isnull(record[field]):
#             return record[field]
#
#
# df["domain"] = df.apply(domain_name, axis=1)
# df = df.drop(columns=["http_host", "sni_host", "dns_host"])
#
# dx = df[~df.domain.str.endswith(".local").fillna(True)]
#
# # Write a csv with the preprocessed data.
# dx.to_csv("preprocessed.csv", index=False)
#
# df = pd.read_csv("preprocessed.csv")
# df.time
# # Letâ€™s build a graph linking
